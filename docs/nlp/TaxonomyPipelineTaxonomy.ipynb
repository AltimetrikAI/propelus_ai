{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12df904",
   "metadata": {},
   "source": [
    "# Profession Taxonomy Mapping Pipeline\n",
    "\n",
    "This notebook assembles an end-to-end workflow for mapping free-text profession titles to a master taxonomy. The code loads the taxonomy tables, engineers lookup vocabularies, builds spaCy matchers, and documents how the classifier consolidates matches into a single primary label. Use this notebook as a reference of the logic that will be implemented on the productive system of taxonomy mamppings and translations.\n",
    "\n",
    "**Workflow outline**\n",
    "- Configure file paths for the profession inputs, input taxonomy\n",
    "- Load datasets with robust CSV readers that handle encoding issues\n",
    "- Generate normalized vocabularies, qualifier lists, and acronym lookups from the taxonomy\n",
    "- Build phrase and token-level matchers that capture strong, suffix-qualified, and prefix-qualified occupations\n",
    "- Run the classifier to assign a primary taxonomy label per input profession and persist the results in a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict8u\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception:\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b00482",
   "metadata": {},
   "source": [
    "## Load raw profession titles\n",
    "\n",
    "Each configured CSV is read with `_read_csv_any`, which retries in Latin-1 if UTF-8 decoding fails. The code picks the appropriate column containing profession or title data, standardizes its name to `text`, concatenates all files, and drops empty records. Inspect the resulting DataFrame to ensure the sample looks as expected before moving on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26eab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILES = [\"./test2.csv\"]\n",
    "TARGET_COLUMN = 'nm_profession'\n",
    "STOPWORDS = set('''the and of in to for a on with by at from or as an amp ii iii iv i v vi vii viii ix x'''.split())\n",
    "CLIENT_TAXONOMY_PATH = \"./master_taxonomy.csv\"\n",
    "LABELS_PATH = \"./labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b6773",
   "metadata": {},
   "source": [
    "## File configuration\n",
    "\n",
    "Set the `INPUT_FILES`, `TARGET_COLUMN`, and taxonomy file paths before running the notebook. The loader will search for profession-like columns if `TARGET_COLUMN` is missing, but explicit configuration is safer when new extracts are introduced. Update the stopword list when the input feed introduces additional filler words that should be ignored during matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebb30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s: str):\n",
    "    s = re.sub(r'[-/]', ' ', s)\n",
    "    s = re.sub(r'[()\\\",;:.\\[\\]{}!?\\u2013\\u2014]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "def tokenize_lower(s: str):\n",
    "    return [t.lower_ for t in nlp.make_doc(normalize_text(s))]\n",
    "def ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "def normalize_acronym(tok: str):\n",
    "    base = tok.lower().replace('.', '').replace('-', '')\n",
    "    for k, v in ACRONYMS.items():\n",
    "        if base == k.replace('-', '').replace('.', ''):\n",
    "            return v\n",
    "    return None\n",
    "def _read_csv_any(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8504b3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223,\n",
       "                                         text\n",
       " 0  Advanced Palliative Hospice Social Worker\n",
       " 1           Associate Clinical Social Worker\n",
       " 2                    Associate Social Worker\n",
       " 3                Baccalaureate Social Worker\n",
       " 4            Bachelors Limited Social Worker)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = []\n",
    "for p in INPUT_FILES:\n",
    "    df = _read_csv_any(p)\n",
    "    col = TARGET_COLUMN if TARGET_COLUMN in df.columns else next((c for c in df.columns if c.lower() in {'nm_profession','profession','title','job_title'} or 'prof' in c.lower() or 'title' in c.lower()), None)\n",
    "    if not col:\n",
    "        raise ValueError(f'No profession/title column in {p}. Columns: {list(df.columns)}')\n",
    "    frames.append(df[[col]].rename(columns={col:'text'}))\n",
    "data = pd.concat(frames, ignore_index=True).dropna(subset=['text']).astype({'text': str})\n",
    "len(data), data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff6afae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client taxonomy shape: (28, 9)\n",
      "Labels gold shape: (28, 2)\n",
      "Client taxonomy columns: ['Taxonomy Code', 'Taxonomy Description', 'Industry', 'Major Group', 'Minor Group', 'Broad Occupation', 'Detailed Occupation', 'Occupation Level', 'Notes / reasoning']\n",
      "Labels columns: ['nm_profession', 'primary_category']\n"
     ]
    }
   ],
   "source": [
    "client_tax = _read_csv_any(CLIENT_TAXONOMY_PATH)\n",
    "labels_gold = _read_csv_any(LABELS_PATH) if Path(LABELS_PATH).exists() else None\n",
    "print(\"Client taxonomy shape:\", client_tax.shape)\n",
    "print(\"Labels gold shape:\", None if labels_gold is None else labels_gold.shape)\n",
    "print(\"Client taxonomy columns:\", list(client_tax.columns))\n",
    "if labels_gold is not None:\n",
    "    print(\"Labels columns:\", list(labels_gold.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23243e3b",
   "metadata": {},
   "source": [
    "## Build taxonomy vocabularies and qualifiers\n",
    "\n",
    "The input taxonomy is normalized to lowercase, whitespace-collapsed strings so we can match consistently. From these cleaned columns, the notebook derives:\n",
    "- Base vocabularies for industry through occupation level\n",
    "- Strong suffix heads sourced from detailed and multiword broad occupations\n",
    "- Qualified suffix heads that need contextual qualifiers\n",
    "- Qualifier phrases gathered from minor groups, industries, and prefixes detected in broad/detailed occupations\n",
    "- Acronym expansions captured from the occupation level column\n",
    "\n",
    "These derived sets are the backbone for the matcher rules that follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eb29c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB sizes: {'Industry': 1, 'Major_Group': 1, 'Minor_Group': 2, 'Broad_Occupation': 2, 'Occupation_Level': 13}\n",
      "Strong heads: 11 | Qualified heads: 12 | Qualifiers: 20 | Acronyms: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Normalize helper ---\n",
    "def _norm_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str).str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    s = s.replace({\"nan\": \"\"})\n",
    "    return s\n",
    "\n",
    "# --- Resolve expected columns (case-insensitive) ---\n",
    "def _resolve(df: pd.DataFrame, keys):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        out[k] = low.get(k, None)\n",
    "    return out\n",
    "\n",
    "need_cols = [\"industry\",\"major group\",\"minor group\",\"broad occupation\",\"detailed occupation\",\"occupation level\"]\n",
    "colmap = _resolve(client_tax, need_cols)\n",
    "\n",
    "IND = _norm_series(client_tax[colmap[\"industry\"]]) if colmap[\"industry\"] else pd.Series([], dtype=str)\n",
    "MAJ = _norm_series(client_tax[colmap[\"major group\"]]) if colmap[\"major group\"] else pd.Series([], dtype=str)\n",
    "MIN = _norm_series(client_tax[colmap[\"minor group\"]]) if colmap[\"minor group\"] else pd.Series([], dtype=str)\n",
    "BRO = _norm_series(client_tax[colmap[\"broad occupation\"]]) if colmap[\"broad occupation\"] else pd.Series([], dtype=str)\n",
    "DET = _norm_series(client_tax[colmap[\"detailed occupation\"]]) if colmap[\"detailed occupation\"] else pd.Series([], dtype=str)\n",
    "LEV = _norm_series(client_tax[colmap[\"occupation level\"]]) if colmap[\"occupation level\"] else pd.Series([], dtype=str)\n",
    "\n",
    "def _uniq_nonempty(s: pd.Series): \n",
    "    return sorted([x for x in s.drop_duplicates().tolist() if x])\n",
    "\n",
    "VOCAB = {\n",
    "    \"Industry\": set(_uniq_nonempty(IND)),\n",
    "    \"Major_Group\": set(_uniq_nonempty(MAJ)),\n",
    "    \"Minor_Group\": set(_uniq_nonempty(MIN)),\n",
    "    \"Broad_Occupation\": set(_uniq_nonempty(BRO)),\n",
    "    \"Occupation_Level\": set(_uniq_nonempty(LEV)),\n",
    "}\n",
    "\n",
    "# Heads strong: detailed + multiword broad\n",
    "STRONG_SUFFIX_HEADS = set(_uniq_nonempty(DET))\n",
    "STRONG_SUFFIX_HEADS |= {b for b in VOCAB[\"Broad_Occupation\"] if len(b.split()) >= 2}\n",
    "\n",
    "# Qualified heads: generic tails\n",
    "GENERIC_TAILS = {\"nurse\",\"therapist\",\"counselor\",\"counsellor\",\"specialist\",\"coordinator\",\"manager\",\"worker\",\"navigator\",\"assistant\",\"associate\"}\n",
    "def _tails(phr):\n",
    "    toks = phr.split()\n",
    "    outs = []\n",
    "    if toks: outs.append(toks[-1])\n",
    "    if len(toks)>=2: outs.append(\" \".join(toks[-2:]))\n",
    "    return outs\n",
    "derived = set()\n",
    "for p in list(VOCAB[\"Broad_Occupation\"]) + list(STRONG_SUFFIX_HEADS):\n",
    "    for t in _tails(p):\n",
    "        if t.split()[-1] in GENERIC_TAILS:\n",
    "            derived.add(t)\n",
    "QUALIFIED_SUFFIX_HEADS = sorted(derived | GENERIC_TAILS)\n",
    "\n",
    "# Qualifiers: from Minor + prefixes of BRO/DET before generic heads + industries\n",
    "QUALIFIERS = set(VOCAB[\"Industry\"] | VOCAB[\"Minor_Group\"])\n",
    "def _prefix_before_head(phrase, head):\n",
    "    if phrase.endswith(head) and phrase != head:\n",
    "        return re.sub(r\"\\s+\", \" \", phrase[:-len(head)]).strip()\n",
    "    return \"\"\n",
    "for p in list(VOCAB[\"Broad_Occupation\"]) + list(STRONG_SUFFIX_HEADS):\n",
    "    for head in QUALIFIED_SUFFIX_HEADS:\n",
    "        if p.endswith(head):\n",
    "            pref = _prefix_before_head(p, head)\n",
    "            if pref: QUALIFIERS.add(pref)\n",
    "\n",
    "# Acronyms: extract uppercase tokens from Occupation Level; keep previous map if exists\n",
    "try:\n",
    "    ACRONYMS = ACRONYMS.copy()\n",
    "except NameError:\n",
    "    ACRONYMS = {}\n",
    "for val in VOCAB[\"Occupation_Level\"]:\n",
    "    for tok in re.split(r\"[ \\-/]\", val):\n",
    "        if tok.isupper() and tok.isalpha() and 2 <= len(tok) <= 6:\n",
    "            ACRONYMS.setdefault(tok.lower(), tok)\n",
    "\n",
    "print(\"VOCAB sizes:\", {k:len(v) for k,v in VOCAB.items()})\n",
    "print(\"Strong heads:\", len(STRONG_SUFFIX_HEADS), \"| Qualified heads:\", len(set(QUALIFIED_SUFFIX_HEADS)), \"| Qualifiers:\", len(QUALIFIERS), \"| Acronyms:\", len(ACRONYMS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55ef3c",
   "metadata": {},
   "source": [
    "## Construct spaCy matchers\n",
    "\n",
    "Phrase matchers cover direct lookups against the normalized vocabularies. The token matcher adds three rule families:\n",
    "- **Strong occupations** where the detailed occupation appears anywhere in the text\n",
    "- **Suffix-qualified patterns** that validate qualifying context before the head term\n",
    "- **Prefix-qualified patterns** that look for qualifiers after the head term\n",
    "\n",
    "Rebuilding these matchers is required whenever the taxonomy-derived vocabularies change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13c3628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matchers rebuilt with prefix+suffix qualified rules.\n"
     ]
    }
   ],
   "source": [
    "phrase_matchers = {}\n",
    "for cat, phrases in VOCAB.items():\n",
    "    pm = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    pm.add(cat, [nlp.make_doc(p) for p in phrases if p])\n",
    "    phrase_matchers[cat] = pm\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Strong (suffix-tolerant; head may appear anywhere)\n",
    "for head in sorted(STRONG_SUFFIX_HEADS):\n",
    "    toks = [{\"LOWER\": t} for t in head.split()]\n",
    "    pattern = [{\"OP\": \"*\"}, *toks]      # .* HEAD\n",
    "    matcher.add(\"Detailed_Occupation__strong__\"+head, [pattern])\n",
    "\n",
    "# Qualified: suffix (require some tokens before head) + prefix (head then tokens)\n",
    "for head in sorted(set(QUALIFIED_SUFFIX_HEADS)):\n",
    "    toks = [{\"LOWER\": t} for t in head.split()]\n",
    "    # suffix-qualified: + HEAD  (then check qualifiers in prefix text)\n",
    "    pattern_suffix = [{\"OP\": \"+\"}, *toks]\n",
    "    matcher.add(\"Detailed_Occupation__qualified_suffix__\"+head, [pattern_suffix])\n",
    "    # prefix-qualified: HEAD +  (then check qualifiers in suffix text)\n",
    "    pattern_prefix = [*toks, {\"OP\": \"+\"}]\n",
    "    matcher.add(\"Detailed_Occupation__qualified_prefix__\"+head, [pattern_prefix])\n",
    "\n",
    "print(\"Matchers rebuilt with prefix+suffix qualified rules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3578b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pick_primary_from_buckets respects your CAT_KEYS order (generic->specific) or the reverse ---\n",
    "try:\n",
    "    CAT_KEYS\n",
    "except NameError:\n",
    "    CAT_KEYS = [\"Industry\",\"Major Group\",\"Minor Group\",\"Broad Occupation\",\"Detailed Occupation\",\"Occupation Level\"]\n",
    "\n",
    "def pick_primary_from_buckets(buckets, prefer_specific=True):\n",
    "    # Choose ONE primary category.\n",
    "    order = list(reversed(CAT_KEYS)) if prefer_specific else CAT_KEYS\n",
    "    for cat in order:\n",
    "        key = cat.replace(\" \", \"_\")\n",
    "        vals = buckets.get(key) or buckets.get(cat)\n",
    "        if vals:\n",
    "            best = max(vals, key=lambda s: len(s.split()))\n",
    "            return cat, best, (\"prefer_specific\" if prefer_specific else \"prefer_generic\")\n",
    "    return None, None, \"no_match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77970c30",
   "metadata": {},
   "source": [
    "## Classify a profession title\n",
    "\n",
    "`classify_text_spacy` orchestrates the final labeling logic:\n",
    "- Normalize the input text and run the phrase and token matchers\n",
    "- Collect matches by category, including qualifier-aware detailed occupations and acronym-based occupation levels\n",
    "- Deduplicate matches and pick a primary label, honoring `CAT_KEYS` order and the `prefer_specific` flag\n",
    "- Return a structured record with the chosen label, the reasoning flag, and all supporting matches\n",
    "\n",
    "This function is the main integration point for downstream services that need taxonomy assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Override classify_text_spacy to use new prefix+suffix qualified logic ---\n",
    "def classify_text_spacy(text: str, prefer_specific=True):\n",
    "    from collections import defaultdict\n",
    "    doc = nlp(normalize_text(text))\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    # Phrase categories\n",
    "    for cat, pm in phrase_matchers.items():\n",
    "        for _, s, e in pm(doc):\n",
    "            buckets[cat].append(doc[s:e].text.lower())\n",
    "\n",
    "    # Detailed Occupation via matcher (strong + qualified suffix/prefix)\n",
    "    for mid, s, e in matcher(doc):\n",
    "        label = nlp.vocab.strings[mid]\n",
    "        span = doc[s:e].text.lower()\n",
    "\n",
    "        if label.startswith(\"Detailed_Occupation__qualified_suffix__\"):\n",
    "            prefix = doc[0:s].text.lower()\n",
    "            if any(q in prefix for q in QUALIFIERS):\n",
    "                buckets[\"Detailed_Occupation\"].append(span)\n",
    "        elif label.startswith(\"Detailed_Occupation__qualified_prefix__\"):\n",
    "            suffix = doc[e:].text.lower()\n",
    "            if any(q in suffix for q in QUALIFIERS):\n",
    "                buckets[\"Detailed_Occupation\"].append(span)\n",
    "        else:\n",
    "            buckets[\"Detailed_Occupation\"].append(span)\n",
    "\n",
    "    # Occupation Level from acronyms (normalize)\n",
    "    for t in doc:\n",
    "        base = t.text.lower().replace(\".\", \"\").replace(\"-\", \"\")\n",
    "        if base in ACRONYMS:\n",
    "            buckets[\"Occupation_Level\"].append(ACRONYMS[base])\n",
    "\n",
    "    # Deduplicate + sort\n",
    "    for k in list(buckets.keys()):\n",
    "        buckets[k] = sorted(set(buckets[k]))\n",
    "\n",
    "    # Decide primary\n",
    "    primary_category, primary_label, reason = pick_primary_from_buckets(buckets, prefer_specific=prefer_specific)\n",
    "\n",
    "    return {\n",
    "        \"source_text\": text,\n",
    "        \"category\": primary_category,\n",
    "        \"label\": primary_label,\n",
    "        \"reason\": reason,\n",
    "        \"matches\": dict(buckets)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324e5bf",
   "metadata": {},
   "source": [
    "## Persist results\n",
    "\n",
    "The notebook applies the classifier to every input profession, writes the structured outputs to `./outputs/taxonomy_mapping.csv`, and logs the destination path. Review this file to validate match quality or feed the results into subsequent QA steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31697f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/germandominguez/Documents/GitHub/propelus_ai/notebooks/outputs\n"
     ]
    }
   ],
   "source": [
    "results = data['text'].apply(classify_text_spacy)\n",
    "OUT = Path('./outputs'); OUT.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(OUT/'taxonomy_mapping.csv', index=False)\n",
    "print('Saved to', OUT.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propelus2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
